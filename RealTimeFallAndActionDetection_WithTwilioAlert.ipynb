{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Action Recognition...\n",
      "Processing video: C:\\Users\\mgree\\Downloads\\Dataset CAUCAFall\\Dataset CAUCAFall\\CAUCAFall\\Subject.8\\Hop\\HopS8.avi\n",
      "FPS: 20.0, Total Frames: 166, Duration: 8.30 seconds\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "Predictions: [[2.9280594e-09 3.0213304e-08 8.1008995e-09 2.3418244e-07 2.7830785e-11\n",
      "  8.5980678e-04 9.9913234e-01 6.2653226e-06 7.2968595e-07 6.8618550e-07]], Predicted Class: 6\n",
      "Frame 50: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predictions: [[6.1423743e-06 5.3439663e-08 1.5287006e-07 2.7985152e-08 1.2748013e-06\n",
      "  3.2065687e-04 9.9950826e-01 3.3107319e-06 8.0787788e-05 7.9344907e-05]], Predicted Class: 6\n",
      "Frame 60: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predictions: [[5.4207555e-04 9.1185420e-06 5.7064808e-05 1.8791847e-05 4.8345432e-06\n",
      "  5.6827134e-06 9.9893743e-01 1.8323371e-05 3.5417988e-04 5.2463027e-05]], Predicted Class: 6\n",
      "Frame 70: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predictions: [[0.10551825 0.00526216 0.0022605  0.0828149  0.00187737 0.01539645\n",
      "  0.75586766 0.00289336 0.01825928 0.00985008]], Predicted Class: 6\n",
      "Frame 80: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predictions: [[2.4452992e-04 6.7523506e-04 3.6774123e-05 4.2597178e-05 3.5506891e-05\n",
      "  6.4988953e-01 3.4293392e-01 4.3726591e-03 1.1173700e-03 6.5186649e-04]], Predicted Class: 5\n",
      "Frame 90: Action: Walk\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predictions: [[3.7540044e-04 1.1999571e-04 2.5361119e-04 2.3216741e-04 3.8677467e-06\n",
      "  9.4936555e-03 9.8722643e-01 1.4630583e-04 1.2518039e-03 8.9686626e-04]], Predicted Class: 6\n",
      "Frame 100: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predictions: [[1.7473030e-04 1.5161307e-05 1.4518366e-05 1.0181102e-05 4.2566640e-08\n",
      "  1.1303943e-03 9.9646962e-01 2.0828815e-03 6.2902429e-05 3.9623897e-05]], Predicted Class: 6\n",
      "Frame 110: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predictions: [[7.1131112e-04 3.8888576e-04 2.7898262e-05 1.8724213e-04 4.0805612e-06\n",
      "  4.9579795e-03 9.9180448e-01 5.5675756e-04 1.1008299e-03 2.6054395e-04]], Predicted Class: 6\n",
      "Frame 120: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predictions: [[3.1387244e-06 3.2476200e-07 5.0149925e-08 1.1632996e-07 2.7192024e-09\n",
      "  2.7444630e-05 9.9993110e-01 2.8458269e-05 7.4429522e-06 1.9928261e-06]], Predicted Class: 6\n",
      "Frame 130: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predictions: [[4.8491784e-06 2.5419015e-06 1.1962864e-07 2.6245839e-06 2.7560425e-09\n",
      "  3.3512997e-04 9.9961346e-01 3.2165078e-06 2.3074468e-05 1.5077404e-05]], Predicted Class: 6\n",
      "Frame 140: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predictions: [[9.8303744e-08 8.4683016e-08 5.4711955e-09 2.1759921e-09 1.8925824e-11\n",
      "  3.4710549e-06 9.9998915e-01 3.0082538e-06 4.2204056e-06 1.7111001e-08]], Predicted Class: 6\n",
      "Frame 150: Action: Hop\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predictions: [[3.3540101e-07 5.3404625e-07 1.0720434e-08 1.3195610e-09 2.2863226e-07\n",
      "  2.5308359e-04 9.9970442e-01 2.8577395e-07 2.7096819e-05 1.3972199e-05]], Predicted Class: 6\n",
      "Frame 160: Action: Hop\n",
      "Actions detected: ['Hop', 'Hop', 'Hop', 'Hop', 'Walk', 'Hop', 'Hop', 'Hop', 'Hop', 'Hop', 'Hop', 'Hop']\n",
      "\n",
      "Starting Fall Detection...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step\n",
      "Fall Detection Result: Non-fall\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from twilio.rest import Client\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Multiply, Add, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define Spatial Attention Layer\n",
    "class SpatialAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=-1, keepdims=True)\n",
    "        \n",
    "        concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "        \n",
    "        attention = self.conv(concat)\n",
    "        \n",
    "        # Apply attention to input\n",
    "        return Multiply()([inputs, attention])\n",
    "\n",
    "# Define Channel Attention Layer\n",
    "class ChannelAttention(Layer):\n",
    "    def __init__(self, reduction_ratio=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.dense1 = Dense(input_shape[-1] // self.reduction_ratio, activation='relu')\n",
    "        self.dense2 = Dense(input_shape[-1], activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "\n",
    "        avg_out = self.dense2(self.dense1(avg_pool))\n",
    "        max_out = self.dense2(self.dense1(max_pool))\n",
    "\n",
    "        # Combine both attention results\n",
    "        attention = Add()([avg_out, max_out])\n",
    "        \n",
    "        return Multiply()([inputs, attention])\n",
    "\n",
    "with custom_object_scope({'SpatialAttention': SpatialAttention, 'ChannelAttention': ChannelAttention}):\n",
    "    action_recognition_model = load_model(\"alternate_attention_model_with_flattening.h5\")\n",
    "fall_detection_model = load_model(\"model_attention.h5\")  \n",
    "\n",
    "ACTION_CATEGORIES = {\n",
    "    0: 'Fall forward',\n",
    "    1: 'Fall backwards',\n",
    "    2: 'Fall left',\n",
    "    3: 'Fall right',\n",
    "    4: 'Fall sitting',\n",
    "    5: 'Walk',\n",
    "    6: 'Hop',\n",
    "    7: 'Pick up object',\n",
    "    8: 'Sit down',\n",
    "    9: 'Kneel'\n",
    "}\n",
    "\n",
    "ACCOUNT_SID = 'ACd5c4ade76d6798103a5331d09c1fe8c2'\n",
    "AUTH_TOKEN = '778069bbac5bb0a8220e562cec67d30c'\n",
    "TWILIO_PHONE = '+14426667159'\n",
    "RECIPIENT_PHONE = '+918555918588'\n",
    "\n",
    "client = Client(ACCOUNT_SID, AUTH_TOKEN)\n",
    "\n",
    "def open_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video file: {video_path}\")\n",
    "    return cap\n",
    "\n",
    "def send_sms_alert(message, to_phone):\n",
    "    try:\n",
    "        client.messages.create(\n",
    "            body=message,\n",
    "            from_=TWILIO_PHONE,\n",
    "            to=to_phone\n",
    "        )\n",
    "        print(\"SMS Alert Sent!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send SMS: {e}\")\n",
    "\n",
    "def predict_action(frame, model):\n",
    "    try:\n",
    "        frame_resized = cv2.resize(frame, (112, 112))  \n",
    "        frame_normalized = frame_resized / 255.0  \n",
    "        frame_batch = np.expand_dims(frame_normalized, axis=0)  # Expand batch dimension\n",
    "\n",
    "        predictions = model.predict(frame_batch)  \n",
    "        predicted_class = np.argmax(predictions)  # Get the predicted class with highest probability\n",
    "        print(f\"Predictions: {predictions}, Predicted Class: {predicted_class}\")\n",
    "        predicted_action = ACTION_CATEGORIES.get(predicted_class, \"Unknown Action\")\n",
    "        return predicted_action, predictions[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting action: {e}\")\n",
    "        return \"Error\", None\n",
    "\n",
    "# Function to predict fall event based on video frames\n",
    "def predict_fall(video_path, model, batch_size=4):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames_batch = []\n",
    "    fall_predictions = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_resized = cv2.resize(frame, (224, 224))  \n",
    "        frames_batch.append(frame_resized)\n",
    "\n",
    "        if len(frames_batch) == batch_size:\n",
    "            batch_array = np.array(frames_batch)\n",
    "            predictions = model.predict(batch_array)  \n",
    "            fall_predictions.extend(np.argmax(predictions, axis=1)) \n",
    "            frames_batch = []  \n",
    "\n",
    "    if len(frames_batch) > 0:\n",
    "        batch_array = np.array(frames_batch)\n",
    "        predictions = model.predict(batch_array)\n",
    "        fall_predictions.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "    cap.release()\n",
    "   \n",
    "    fall_count = fall_predictions.count(1)\n",
    "    non_fall_count = fall_predictions.count(0)\n",
    "    final_prediction = \"Fall\" if fall_count > non_fall_count else \"Non-fall\"\n",
    "    return final_prediction\n",
    "\n",
    "def process_video_for_action(video_path, model, start_frame=40, frame_skip=10):\n",
    "    cap = open_video(video_path)  \n",
    "    frame_count = 0\n",
    "    actions = []\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get video frames per second\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total number of frames in the video\n",
    "    total_duration = total_frames / fps  # Total duration of video in seconds\n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"FPS: {fps}, Total Frames: {total_frames}, Duration: {total_duration:.2f} seconds\")\n",
    "\n",
    "    output_path = \"output_with_action_labels.avi\"  \n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  \n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  \n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))  # Create video writer\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  \n",
    "\n",
    "        # Predict action every `frame_skip` frames starting from `start_frame`\n",
    "        if frame_count >= start_frame and frame_count % frame_skip == 0:\n",
    "            predicted_action, _ = predict_action(frame, model)  # Get predicted action for the frame\n",
    "            actions.append(predicted_action) \n",
    "\n",
    "            # Add predicted label to the frame\n",
    "            label = f\"Action: {predicted_action}\"\n",
    "            print(f\"Frame {frame_count}: {label}\") \n",
    "\n",
    "            # Draw label on the frame\n",
    "            text_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "            text_width, text_height = text_size\n",
    "            cv2.rectangle(frame, (10, 10), (10 + text_width, 30 + text_height), (0, 0, 0), -1)  \n",
    "            cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2) \n",
    "\n",
    "        out.write(frame) \n",
    "        cv2.imshow(\"Action Recognition\", frame) \n",
    "\n",
    "        if cv2.waitKey(300) & 0xFF == ord('q'):  # Exit on 'q' key\n",
    "            print(\"Video processing interrupted by user.\")\n",
    "            break\n",
    "\n",
    "        if frame_count > (fps * 300):  \n",
    "            print(\"Stopping after 50 seconds of playback.\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release() \n",
    "    out.release()  \n",
    "    cv2.destroyAllWindows() \n",
    "    return actions\n",
    "\n",
    "def process_video_pipeline(video_path):\n",
    "    try:\n",
    "        print(\"Starting Action Recognition...\")\n",
    "        actions = process_video_for_action(video_path, action_recognition_model)\n",
    "        print(f\"Actions detected: {actions}\")\n",
    "\n",
    "        print(\"\\nStarting Fall Detection...\")\n",
    "        fall_prediction = predict_fall(video_path, fall_detection_model)\n",
    "        print(f\"Fall Detection Result: {fall_prediction}\")\n",
    "\n",
    "        if fall_prediction == \"Fall\":\n",
    "            alert_message = f\"Alert! A fall has been detected.\"\n",
    "            send_sms_alert(alert_message, RECIPIENT_PHONE)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in processing pipeline: {e}\")\n",
    "\n",
    "video_path = r\"C:\\Users\\mgree\\Downloads\\F1.avi\"\n",
    "process_video_pipeline(video_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
